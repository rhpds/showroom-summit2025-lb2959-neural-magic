{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7854d876-3cd1-4a18-bf9c-4c947166fd88",
   "metadata": {},
   "source": [
    "# `int8` Weight and Activation Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800f926-dc4c-42a1-84a7-03be28d7463f",
   "metadata": {},
   "source": [
    "`llm-compressor` supports quantizing weights and activations to `int` for memory savings and inference acceleration with `vLLM`\n",
    "Note: `int8` compuation is supported on Nvidia GPUs with compute capability > 7.5 (Turing, Ampere, Ada Lovelace, Hopper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99081-3a8b-486c-9882-deb387c83850",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b244-10f5-4aac-981a-3a07864e0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e9b1-1e04-4c1c-be37-617c489b2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep llmcompressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccee8ff-8b6d-444b-be09-7f3964142cb6",
   "metadata": {},
   "source": [
    "### Other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893ef37-287d-4f13-87de-e063ae670178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6f712-59e1-4b03-874d-540836330c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24bf14-c77b-4f07-851d-d70cc61646eb",
   "metadata": {},
   "source": [
    "## Quantize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b5b-b309-4c23-a245-70d78d0955fc",
   "metadata": {},
   "source": [
    "There are 4 steps:\n",
    "1. Load model\n",
    "2. Prepare calibration data\n",
    "3. Apply quantization\n",
    "4. Evaluate accuracy in vLLM\n",
    "5. Upload model to S3 (MinIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad553f-1f6f-4583-b280-6205930debe8",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99299f19",
   "metadata": {},
   "source": [
    "Load the model using AutoModelForCausalLM for handling quantized saving and loading.\n",
    "\n",
    "The model can be loaded from HuggingFace using something like the next:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "```\n",
    "\n",
    "But to save time on the lab, we have prepared a MinIO bucket that already contains the model. \n",
    "\n",
    "As this workbench was created with a dataconnection attached, the required env vars for accessing the MinIO S3 bucket are defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf85cf99",
   "metadata": {},
   "source": [
    "NOTE: If you already executed this for the previous ipynb, the model is already downloaded and you can skip this (downloading) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7f96-adfd-4c39-8e58-e78aa9e99a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_NAME = \"base_model\"\n",
    "MODEL_DOWNLOAD_PATH = \"/opt/app-root/src/base_model\"\n",
    "\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False)\n",
    "\n",
    "# list all objects in the folder\n",
    "objects = s3_client.list_objects(Bucket=s3_bucket_name, Prefix=MODEL_NAME)\n",
    "\n",
    "# download each object in the folder\n",
    "for object in objects['Contents']:\n",
    "    file_name = object['Key']\n",
    "    local_file_name = os.path.join(MODEL_DOWNLOAD_PATH, file_name.replace(MODEL_NAME, '')[1:])\n",
    "    if not os.path.exists(os.path.dirname(local_file_name)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(local_file_name))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                print(\"Error downloading model\")\n",
    "                raise\n",
    "    s3_client.download_file(s3_bucket_name, file_name, local_file_name)\n",
    "\n",
    "print('Model downloaded successfully from S3.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742346d-f226-4462-871a-bf4008a04091",
   "metadata": {},
   "source": [
    "### Prepare calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdaadd-11b8-4627-be3d-3a1ffa4c9f88",
   "metadata": {},
   "source": [
    "Prepare the calibration data. When quantizing weigths of a model to int4 using GPTQ, we need some sample data to run the GPTQ algorithms. As a result, it is very useful to use calibration data that closely matches the type of data used in deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.\n",
    "\n",
    "In our case, we are quantizing an Instruction tuned generic model, so we will use the ultrachat dataset. Some best practices include:\n",
    "- 512 samples is a good place to start (increase if accuracy drops)\n",
    "- 2048 sequence length is a good place to start\n",
    "- Use the chat template or instrucion template that the model is trained with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b1c6-f408-4ec5-81a0-3e8ae8019070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 256  # 1024\n",
    "DATASET_ID = \"neuralmagic/LLM_compression_calibration\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    #concat_txt = example[\"instruction\"] + \"\\n\" + example[\"output\"]\n",
    "    #return {\"text\": concat_txt}\n",
    "    return {\"text\": example[\"text\"]}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fdf9-5963-46c6-b365-4549db481f77",
   "metadata": {},
   "source": [
    "### Apply quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c889b-b827-4f8d-be34-2cc05bba48f1",
   "metadata": {},
   "source": [
    "With the dataset ready, we will now apply quantization.\n",
    "\n",
    "We first select the quantization algorithm. For W8A8, we want to:\n",
    "- Run SmoothQuant to make the activations easier to quantize\n",
    "- Quantize the weights to 8 bits with channelwise scales using GPTQ\n",
    "- Quantize the activations with dynamic per token strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6d53f-8db5-40a1-874f-14d60b007b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.transformers import oneshot\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "\n",
    "DAMPENING_FRAC = 0.1  # 0.01\n",
    "OBSERVER = \"mse\"  # minmax\n",
    "GROUP_SIZE = 128  # 64\n",
    "# Configure the quantization algorithm to run.\n",
    "ignore=[\"lm_head\"]\n",
    "mappings=[\n",
    "    [[\"re:.*q_proj\", \"re:.*k_proj\", \"re:.*v_proj\"], \"re:.*input_layernorm\"],\n",
    "    [[\"re:.*gate_proj\", \"re:.*up_proj\"], \"re:.*post_attention_layernorm\"],\n",
    "    [[\"re:.*down_proj\"], \"re:.*up_proj\"]\n",
    "]\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.7, ignore=ignore, mappings=mappings),\n",
    "    GPTQModifier(\n",
    "        targets=[\"Linear\"],\n",
    "        ignore=ignore,\n",
    "        scheme=\"W8A8\",\n",
    "        dampening_frac=DAMPENING_FRAC,\n",
    "        observer=OBSERVER,\n",
    "    )\n",
    "]\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    max_seq_length=8196,\n",
    ")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W8A8\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed216fb-9c28-4bff-8033-b68c744b4cd9",
   "metadata": {},
   "source": [
    "### Evaluate accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cc91-dfca-4dc1-b150-692e3a309ad3",
   "metadata": {},
   "source": [
    "We can evaluate accuracy with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737002c6-1512-402f-adfa-965660478ece",
   "metadata": {},
   "source": [
    "##### Check GPU memory leftovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf001-f52b-46a2-a034-1a5ac99d8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fa79-3e05-4f83-90b7-dd7be66513dc",
   "metadata": {},
   "source": [
    "If not available, restart the kernel and start from here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5eab2-e7f1-4631-b74f-8e5b44d06dc2",
   "metadata": {},
   "source": [
    "#### Install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a3ad8-effc-4212-9c5d-ab09d772e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lm_eval==v0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facbbf2-400c-424f-bada-4f224d4f5fd0",
   "metadata": {},
   "source": [
    "#### Run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d660bb-9435-4d1c-9599-7e9f8f1ca4ff",
   "metadata": {},
   "source": [
    "Run the following to test accuracy on GSM-8K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd85be-edf1-4736-86eb-fd93c09c146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W8A8\"\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,add_bos_token=true \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b570baf-46e1-46e9-b970-c3f156814a69",
   "metadata": {},
   "source": [
    "If powerfull GPU(s), you could also run the OpenLLM with the following:\n",
    "```bash\n",
    "!lm_eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks openllm \\\n",
    "  --write_out \\\n",
    "  --batch_size auto \\\n",
    "  --output_path output_dir \\\n",
    "  --show_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a1582",
   "metadata": {},
   "source": [
    "### Upload Optimized Model to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e422-0ea3-41df-97cd-36cbcf4e5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "OPTIMIZED_MODEL_DIR = MODEL_ID.split(\"/\")[-1] + \"-W8A8\"\n",
    "S3_PATH = \"granite-int8\"\n",
    "\n",
    "print('Starting results upload.')\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "print(f'Uploading predictions to bucket {s3_bucket_name} '\n",
    "        f'to S3 storage at {s3_endpoint_url}')\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False\n",
    ")\n",
    "\n",
    "# Walk through the local folder and upload files\n",
    "for root, dirs, files in os.walk(OPTIMIZED_MODEL_DIR):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        s3_file_path = os.path.join(S3_PATH, local_file_path[len(OPTIMIZED_MODEL_DIR)+1:])\n",
    "        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)\n",
    "        print(f'Uploaded {local_file_path}')\n",
    "\n",
    "print('Finished uploading results.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
