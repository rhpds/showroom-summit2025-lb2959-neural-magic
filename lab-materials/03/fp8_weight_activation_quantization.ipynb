{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7854d876-3cd1-4a18-bf9c-4c947166fd88",
   "metadata": {},
   "source": [
    "# `fp8` Weight and Activation Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800f926-dc4c-42a1-84a7-03be28d7463f",
   "metadata": {},
   "source": [
    "`llmcompressor` supports quantizing weights and activations to `fp8` for memory savings and inference acceleration with `vllm`\n",
    "\n",
    "Note: `fp8` computation is supported on Nvidia GPUs with compute capability > 8.9 (Ada Lovelace, Hopper).\n",
    "\n",
    "Note ðŸš¨: The steps here will take around 10 minutes, depending on the connectivity. The most time consumming steps are the installation of llmcompressor and the quantization step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99081-3a8b-486c-9882-deb387c83850",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91922afa",
   "metadata": {},
   "source": [
    "Installing llmcompressor may take a minute, depending on the bandwith available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b244-10f5-4aac-981a-3a07864e0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e9b1-1e04-4c1c-be37-617c489b2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep llmcompressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccee8ff-8b6d-444b-be09-7f3964142cb6",
   "metadata": {},
   "source": [
    "### Other dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fea8c1f",
   "metadata": {},
   "source": [
    "The next command may show the next ERRORs that can be dismiss:\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "llmcompressor 0.5.0 requires transformers<4.50,>4.0, but you have transformers 4.51.3 which is incompatible.\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "llmcompressor 0.5.1 requires compressed-tensors==0.9.4, but you have compressed-tensors 0.9.3 which is incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893ef37-287d-4f13-87de-e063ae670178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate vllm boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24bf14-c77b-4f07-851d-d70cc61646eb",
   "metadata": {},
   "source": [
    "## Quantize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b5b-b309-4c23-a245-70d78d0955fc",
   "metadata": {},
   "source": [
    "There are 4 steps:\n",
    "1. Load model\n",
    "2. Apply quantization\n",
    "3. Evaluate accuracy in vLLM\n",
    "4. Upload model to S3 (MinIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad553f-1f6f-4583-b280-6205930debe8",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0e149-fc44-478f-aece-bc61adb9f9ad",
   "metadata": {},
   "source": [
    "Load the model using AutoModelForCausalLM for handling quantized saving and loading.\n",
    "\n",
    "The model can be loaded from HuggingFace using something like the next:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "```\n",
    "\n",
    "But to save time on the lab, we have prepared a MinIO bucket that already contains the model. \n",
    "\n",
    "As this workbench was created with a dataconnection attached, the required env vars for accessing the MinIO S3 bucket are defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4727fc",
   "metadata": {},
   "source": [
    "NOTE: If you already executed this for the previous ipynb, the model is already downloaded and you can skip this (downloading) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7f96-adfd-4c39-8e58-e78aa9e99a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_NAME = \"base_model\"\n",
    "MODEL_DOWNLOAD_PATH = \"/opt/app-root/src/base_model\"\n",
    "\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False)\n",
    "\n",
    "# list all objects in the folder\n",
    "objects = s3_client.list_objects(Bucket=s3_bucket_name, Prefix=MODEL_NAME)\n",
    "\n",
    "# download each object in the folder\n",
    "for object in objects['Contents']:\n",
    "    file_name = object['Key']\n",
    "    local_file_name = os.path.join(MODEL_DOWNLOAD_PATH, file_name.replace(MODEL_NAME, '')[1:])\n",
    "    if not os.path.exists(os.path.dirname(local_file_name)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(local_file_name))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                print(\"Error downloading model\")\n",
    "                raise\n",
    "    s3_client.download_file(s3_bucket_name, file_name, local_file_name)\n",
    "\n",
    "print('Model downloaded successfully from S3.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fdf9-5963-46c6-b365-4549db481f77",
   "metadata": {},
   "source": [
    "### Apply quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c889b-b827-4f8d-be34-2cc05bba48f1",
   "metadata": {},
   "source": [
    "For `fp8` quantization, we can recover accuracy with simple PTQ quantization.\n",
    "\n",
    "We recommend targeting all `Linear` layers using the `FP8_DYNAMIC` scheme, which uses:\n",
    "- Static, per-channel quantization on the weights\n",
    "- Dynamic, per-token quantization on the activations\n",
    "\n",
    "Since simple PTQ does not require data for weight quantization and the activations are quantized dynamically, we do not need any calibration data for this quantization flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6d53f-8db5-40a1-874f-14d60b007b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import QuantizationModifier\n",
    "from llmcompressor.transformers import oneshot\n",
    "\n",
    "# Configure the simple PTQ quantization\n",
    "recipe = QuantizationModifier(\n",
    "  targets=\"Linear\", scheme=\"FP8_DYNAMIC\", ignore=[\"lm_head\"])\n",
    "\n",
    "# Apply the quantization algorithm.\n",
    "oneshot(model=model, recipe=recipe)\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-FP8-Dynamic\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed216fb-9c28-4bff-8033-b68c744b4cd9",
   "metadata": {},
   "source": [
    "### Evaluate accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cc91-dfca-4dc1-b150-692e3a309ad3",
   "metadata": {},
   "source": [
    "We can evaluate accuracy with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737002c6-1512-402f-adfa-965660478ece",
   "metadata": {},
   "source": [
    "##### Check GPU memory leftovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf001-f52b-46a2-a034-1a5ac99d8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fa79-3e05-4f83-90b7-dd7be66513dc",
   "metadata": {},
   "source": [
    "IMPORTANT: ðŸš¨ After quantizing the model the GPU memory may not be freed (see the above output). You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5eab2-e7f1-4631-b74f-8e5b44d06dc2",
   "metadata": {},
   "source": [
    "#### Install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a3ad8-effc-4212-9c5d-ab09d772e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lm_eval==v0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facbbf2-400c-424f-bada-4f224d4f5fd0",
   "metadata": {},
   "source": [
    "#### Run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d660bb-9435-4d1c-9599-7e9f8f1ca4ff",
   "metadata": {},
   "source": [
    "Run the following to test accuracy on GSM-8K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd85be-edf1-4736-86eb-fd93c09c146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-FP8-Dynamic\"\n",
    "\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,add_bos_token=true,max_model_len=6144 \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b570baf-46e1-46e9-b970-c3f156814a69",
   "metadata": {},
   "source": [
    "If powerfull GPU(s), you could also run the OpenLLM with the following:\n",
    "```bash\n",
    "!lm_eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks openllm \\\n",
    "  --write_out \\\n",
    "  --batch_size auto \\\n",
    "  --output_path output_dir \\\n",
    "  --show_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafa71e",
   "metadata": {},
   "source": [
    "### Upload Optimized Model to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1e422-0ea3-41df-97cd-36cbcf4e5520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "OPTIMIZED_MODEL_DIR = MODEL_ID.split(\"/\")[-1] + \"-FP8-Dynamic\"\n",
    "S3_PATH = \"granite-fp8\"\n",
    "\n",
    "print('Starting results upload.')\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "print(f'Uploading predictions to bucket {s3_bucket_name} '\n",
    "        f'to S3 storage at {s3_endpoint_url}')\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False\n",
    ")\n",
    "\n",
    "# Walk through the local folder and upload files\n",
    "for root, dirs, files in os.walk(OPTIMIZED_MODEL_DIR):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        s3_file_path = os.path.join(S3_PATH, local_file_path[len(OPTIMIZED_MODEL_DIR)+1:])\n",
    "        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)\n",
    "        print(f'Uploaded {local_file_path}')\n",
    "\n",
    "print('Finished uploading results.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
