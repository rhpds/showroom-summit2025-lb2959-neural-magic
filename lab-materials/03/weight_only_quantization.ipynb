{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7854d876-3cd1-4a18-bf9c-4c947166fd88",
   "metadata": {},
   "source": [
    "# `int4` Weight Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800f926-dc4c-42a1-84a7-03be28d7463f",
   "metadata": {},
   "source": [
    "`llm-compressor` supports quantizing weights to `int4` for memory savings and inference acceleration with `vLLM`.\n",
    "\n",
    "Note: `int4` mixed precision computation is supported on Nvidia GPUs with compute capability > 8.0 (Ampere, Ada Lovelace, Hopper).\n",
    "\n",
    "Note ðŸš¨: The steps here will take around 20 minutes, depending on the connectivity. The most time consumming steps are the installation of llmcompressor (up to 5 mins) and the quantization step (which can take more than 10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef99081-3a8b-486c-9882-deb387c83850",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065d582f",
   "metadata": {},
   "source": [
    "Installing llmcompressor may take a minute, depending on the bandwith available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b244-10f5-4aac-981a-3a07864e0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llmcompressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e9e9b1-1e04-4c1c-be37-617c489b2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep llmcompressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccee8ff-8b6d-444b-be09-7f3964142cb6",
   "metadata": {},
   "source": [
    "### Other dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d94456",
   "metadata": {},
   "source": [
    "The next command may show the next ERRORs that can be dismiss:\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "llmcompressor 0.5.0 requires transformers<4.50,>4.0, but you have transformers 4.51.3 which is incompatible.\n",
    "\n",
    "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
    "llmcompressor 0.5.1 requires compressed-tensors==0.9.4, but you have compressed-tensors 0.9.3 which is incompatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0893ef37-287d-4f13-87de-e063ae670178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate vllm boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd24bf14-c77b-4f07-851d-d70cc61646eb",
   "metadata": {},
   "source": [
    "## Quantize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55327b5b-b309-4c23-a245-70d78d0955fc",
   "metadata": {},
   "source": [
    "There are 5 steps:\n",
    "1. Load model\n",
    "2. Prepare calibration data\n",
    "3. Apply quantization\n",
    "4. Evaluate accuracy in vLLM\n",
    "5. Upload model to S3 (MinIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad553f-1f6f-4583-b280-6205930debe8",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777ca181",
   "metadata": {},
   "source": [
    "Load the model using AutoModelForCausalLM for handling quantized saving and loading.\n",
    "\n",
    "The model can be loaded from HuggingFace using something like the next:\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"ibm-granite/granite-3.2-2b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "```\n",
    "\n",
    "But to save time on the lab, we have prepared a MinIO bucket that already contains the model. \n",
    "\n",
    "As this workbench was created with a dataconnection attached, the required env vars for accessing the MinIO S3 bucket are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import errno\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_NAME = \"base_model\"\n",
    "MODEL_DOWNLOAD_PATH = \"/opt/app-root/src/base_model\"\n",
    "\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False)\n",
    "\n",
    "# list all objects in the folder\n",
    "objects = s3_client.list_objects(Bucket=s3_bucket_name, Prefix=MODEL_NAME)\n",
    "\n",
    "# download each object in the folder\n",
    "for object in objects['Contents']:\n",
    "    file_name = object['Key']\n",
    "    local_file_name = os.path.join(MODEL_DOWNLOAD_PATH, file_name.replace(MODEL_NAME, '')[1:])\n",
    "    if not os.path.exists(os.path.dirname(local_file_name)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(local_file_name))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                print(\"Error downloading model\")\n",
    "                raise\n",
    "    s3_client.download_file(s3_bucket_name, file_name, local_file_name)\n",
    "\n",
    "print('Model downloaded successfully from S3.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af7f96-adfd-4c39-8e58-e78aa9e99a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, device_map=\"auto\", torch_dtype=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742346d-f226-4462-871a-bf4008a04091",
   "metadata": {},
   "source": [
    "### Prepare calibration data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bdaadd-11b8-4627-be3d-3a1ffa4c9f88",
   "metadata": {},
   "source": [
    "Prepare the calibration data. When quantizing weigths of a model to int4 using GPTQ, we need some sample data to run the GPTQ algorithms. As a result, it is very useful to use calibration data that closely matches the type of data used in deployment. If you have fine-tuned a model, using a sample of your training data is a good idea.\n",
    "\n",
    "In our case, we are quantizing an Instruction tuned generic model, so we will use the ultrachat dataset. Some best practices include:\n",
    "- 512 samples is a good place to start (increase if accuracy drops). We are going to use 256 to speed up the process.\n",
    "- 2048 sequence length is a good place to start\n",
    "- Use the chat template or instrucion template that the model is trained with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c3b1c6-f408-4ec5-81a0-3e8ae8019070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "NUM_CALIBRATION_SAMPLES = 256  # 1024\n",
    "DATASET_ID = \"neuralmagic/LLM_compression_calibration\"\n",
    "DATASET_SPLIT = \"train\"\n",
    "\n",
    "# Load dataset.\n",
    "ds = load_dataset(DATASET_ID, split=DATASET_SPLIT)\n",
    "ds = ds.shuffle(seed=42).select(range(NUM_CALIBRATION_SAMPLES))\n",
    "\n",
    "# Preprocess the data into the format the model is trained with.\n",
    "def preprocess(example):\n",
    "    #concat_txt = example[\"instruction\"] + \"\\n\" + example[\"output\"]\n",
    "    #return {\"text\": concat_txt}\n",
    "    return {\"text\": example[\"text\"]}\n",
    "ds = ds.map(preprocess)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "ds = ds.map(tokenize, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376fdf9-5963-46c6-b365-4549db481f77",
   "metadata": {},
   "source": [
    "### Apply quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c889b-b827-4f8d-be34-2cc05bba48f1",
   "metadata": {},
   "source": [
    "With the dataset ready, we will now apply quantization.\n",
    "\n",
    "We first select the quantization algorithm.\n",
    "\n",
    "In our case, we will apply the default GPTQ recipe for int4 (which uses static group size 128 scales) to all linear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2175286",
   "metadata": {},
   "source": [
    "NOTE: ðŸš¨ The quantization step takes a long time to complete due to the callibration requirements -- around 10 mins, depending on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f6d53f-8db5-40a1-874f-14d60b007b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor.transformers import oneshot\n",
    "\n",
    "DAMPENING_FRAC = 0.1  # 0.01\n",
    "OBSERVER = \"mse\"  # minmax\n",
    "GROUP_SIZE = 128  # 64\n",
    "# Configure the quantization algorithm to run.\n",
    "recipe = [\n",
    "    GPTQModifier(\n",
    "        targets=[\"Linear\"],\n",
    "        ignore=[\"lm_head\"],\n",
    "        scheme=\"w4a16\",\n",
    "        dampening_frac=DAMPENING_FRAC,\n",
    "        observer=OBSERVER,\n",
    "        group_size=GROUP_SIZE\n",
    "    )\n",
    "]\n",
    "\n",
    "# Apply quantization.\n",
    "oneshot(\n",
    "    model=model,\n",
    "    dataset=ds,\n",
    "    recipe=recipe,\n",
    "    num_calibration_samples=NUM_CALIBRATION_SAMPLES,\n",
    "    max_seq_length=8196,\n",
    ")\n",
    "\n",
    "# Save to disk compressed.\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W4A16\"\n",
    "model.save_pretrained(SAVE_DIR, save_compressed=True)\n",
    "tokenizer.save_pretrained(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed216fb-9c28-4bff-8033-b68c744b4cd9",
   "metadata": {},
   "source": [
    "### Evaluate accuracy in vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02cc91-dfca-4dc1-b150-692e3a309ad3",
   "metadata": {},
   "source": [
    "We can evaluate accuracy with lm_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737002c6-1512-402f-adfa-965660478ece",
   "metadata": {},
   "source": [
    "##### Check GPU memory leftovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119cf001-f52b-46a2-a034-1a5ac99d8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e2fa79-3e05-4f83-90b7-dd7be66513dc",
   "metadata": {},
   "source": [
    "IMPORTANT: ðŸš¨ After quantizing the model the GPU memory may not be freed (see the above output). You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5eab2-e7f1-4631-b74f-8e5b44d06dc2",
   "metadata": {},
   "source": [
    "#### Install lm_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a3ad8-effc-4212-9c5d-ab09d772e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q lm_eval==v0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facbbf2-400c-424f-bada-4f224d4f5fd0",
   "metadata": {},
   "source": [
    "#### Run the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d660bb-9435-4d1c-9599-7e9f8f1ca4ff",
   "metadata": {},
   "source": [
    "Run the following to test accuracy on GSM-8K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd85be-edf1-4736-86eb-fd93c09c146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "SAVE_DIR = MODEL_ID.split(\"/\")[-1] + \"-W4A16\"\n",
    "!lm_eval --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR,add_bos_token=true,max_model_len=6144 \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks gsm8k \\\n",
    "  --num_fewshot 5 \\\n",
    "  --limit 250 \\\n",
    "  --batch_size 'auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b570baf-46e1-46e9-b970-c3f156814a69",
   "metadata": {},
   "source": [
    "If powerfull GPU(s), you could also run the OpenLLM with the following:\n",
    "```bash\n",
    "!lm_eval \\\n",
    "  --model vllm \\\n",
    "  --model_args pretrained=$SAVE_DIR\",dtype=auto,add_bos_token=True,max_model_len=4096,tensor_parallel_size=1,gpu_memory_utilization=0.8,enable_chunked_prefill=True \\\n",
    "  --trust_remote_code \\\n",
    "  --tasks openllm \\\n",
    "  --write_out \\\n",
    "  --batch_size auto \\\n",
    "  --output_path output_dir \\\n",
    "  --show_config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ce381",
   "metadata": {},
   "source": [
    "### Upload Optimized Model to MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f041f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from boto3 import client\n",
    "\n",
    "MODEL_ID = \"/opt/app-root/src/base_model\"\n",
    "OPTIMIZED_MODEL_DIR = MODEL_ID.split(\"/\")[-1] + \"-W4A16\"\n",
    "S3_PATH = \"granite-int4\"\n",
    "\n",
    "print('Starting results upload.')\n",
    "s3_endpoint_url = os.environ[\"AWS_S3_ENDPOINT\"]\n",
    "s3_access_key = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "s3_secret_key = os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    "s3_bucket_name = os.environ[\"AWS_S3_BUCKET\"]\n",
    "\n",
    "print(f'Uploading predictions to bucket {s3_bucket_name} '\n",
    "        f'to S3 storage at {s3_endpoint_url}')\n",
    "\n",
    "s3_client = client(\n",
    "    's3', endpoint_url=s3_endpoint_url, aws_access_key_id=s3_access_key,\n",
    "    aws_secret_access_key=s3_secret_key, verify=False\n",
    ")\n",
    "\n",
    "# Walk through the local folder and upload files\n",
    "for root, dirs, files in os.walk(OPTIMIZED_MODEL_DIR):\n",
    "    for file in files:\n",
    "        local_file_path = os.path.join(root, file)\n",
    "        s3_file_path = os.path.join(S3_PATH, local_file_path[len(OPTIMIZED_MODEL_DIR)+1:])\n",
    "        s3_client.upload_file(local_file_path, s3_bucket_name, s3_file_path)\n",
    "        print(f'Uploaded {local_file_path}')\n",
    "\n",
    "print('Finished uploading results.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
