= Weights and Activation Quantization (FP8)

In this exercise, we will use a notebook to investigate how LLMs weights and activations can be quantized to **FP8** for memory savings and inference acceleration. This quantization method allows for:

- A **2x reduction** in model memory requirements
- Up to a **1.6x improvement** in throughput with minimal impact on accuracy

IMPORTANT: ðŸš¨ Currently, only Hopper and Ada Lovelace GPUs are officially supported for W8A8. Ampere GPUs are supported for W8A16 (weight-only FP8) utilizing Marlin kernels.

== Quantization Process

The quantization process involves the following steps:

1. **Load the Model**: Load the pre-trained LLM model.
2. **Quantize the Model**: Convert the model weights and activations to **FP8** format.
   ** Using RTN (FP8_dynamic)
   ** No need for calibration data
3. **Evaluate the Model**: Evaluate the quantized model's accuracy.

IMPORTANT: ðŸš¨ After quantizing the model, the GPU memory may not be freed. You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available.
[.bordershadow]
image::03/03-restart-kernel.png[title="Restart Kernel", link=self, window=blank, width=100%]

== Exercise: Quantize the Model with llm-compressor

Go to the workbench created in the previous section (Section 2). From the `showroom-summit2025-lb2959-neural-magic/lab-materials/03` folder, please open the notebook called `fp8_weight_activation_quantization.ipynb` and follow the instructions.
[.bordershadow]
image::03/03-03-fp8-notebook.png[title="Notebook", link=self, window=blank, width=100%]

To execute the cells you can select them and either click on the **play** icon or press **Shift + Enter**
[.bordershadow]
image::03/03-execute-cell.png[title="Execute Cell", link=self, window=blank, width=100%]

When the cell is being executed, you can see **[*]**. And once the execution has completed, you will see a number instead of the *, e.g., **[1]**
[.bordershadow]
image::03/03-cell-status.png[title="Cell Status", link=self, window=blank, width=100%]

When done, you can close the notebook and head to the next page.

IMPORTANT: ðŸš¨ Once you complete all the quantization exercises and you no longer need the workbench, ensure you **stop it** so that the associated GPU gets freed and can be utilized to serve the model.
[.bordershadow]
image::03/03-workbench-done.png[title="Workbench Done", link=self, window=blank, width=100%]
[.bordershadow]
image::03/03-workbench-stop.png[title="Workbench Stop", link=self, window=blank, width=100%]