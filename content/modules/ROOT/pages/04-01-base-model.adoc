= Deploy and Test the Base Model

== Deploy the Base Model with VLLM

Ready to deploy your model? Letâ€™s get started! Follow these steps to bring your model to life in the Data Science Project (`{user}`):

. **Navigate to Your Project**: Head over to your created Data Science Project and locate the **Models** section. 

. **Select the Service Platform**: Click on **Models** and choose the **Single-model service platform** option.
+
[.bordershadow]
image::04/04-01-single-model.png[link=self, window=blank, width=100%]

. **Deploy Your Model**: Click on the **Deploy model** button to start the deployment process.
+
[.bordershadow]
image::04/04-01-deploy.png[link=self, window=blank, width=100%]

. **Fill Out the Deployment Form**: Youâ€™ll need to provide some essential information. Hereâ€™s what to enter:
   * **Name**: `base`
   * **Serving runtime**: `vLLM ServingRuntime for KServe`
   * **Model server size**: `Small`
   * **Accelerator**: `NVIDIA GPU`
   * **Model route**: Select the option to make your model available through an external route.
   * **Token authentication**: Choose `Require token authentication` and leave the default **Service account name**.
   * **Existing connection**:
   ** **Connection**: `Minio - models`
   ** **Path**: `base_model`
+
[.bordershadow]
image::04/04-01-model-inputs.png[link=self, window=blank, width=100%]

. **Deploy and Wait**: After filling out the form, click on **Deploy**. Now, wait while your model gets ready. This might take a moment! â˜•
+
[.bordershadow]
image::04/04-01-model-status.png[link=self, window=blank, width=100%]


== Test the Base Model

Congratulations on successfully deploying your model! ðŸŽ‰ Now, it's time to put it to the test. Get ready to send a request to your model and measure its response time. Let's dive in!

=== Workbench Setup

Similarly to Section 2.3, go to **Data Science Projects**, select your previously created project (`{user}`), and:

* Click on **Create a workbench** to create a new workbench, named `terminal`. This time without attaching any GPU. Also, the `small` size should be enough.
+
[.bordershadow]
image::04/04-wb-list.png[link=self, window=blank, width=100%]
[.bordershadow]
image::04/04-wb-creation.png[link=self, window=blank, width=100%]
[.bordershadow]
image::04/04-wb-ready.png[link=self, window=blank, width=100%]

* Open the `terminal` workbench and create a terminal inside.
+
[.bordershadow]
image::04/04-wb-terminal.png[link=self, window=blank, width=100%]

* **Clone the repository** `{git-clone-repo-url}` and go to the `{git-clone-repo-name}/lab-materials/04` folder.
+
[.bordershadow]
image::04/04-wb-clone.png[link=self, window=blank, width=100%]

* **Update Your Variables**: Open the `request.py` file and update the following variables to match your setup:
+
[.bordershadow]
image::04/04-wb-request.png[link=self, window=blank, width=100%]
[source,python]
----
MODEL = "your-model-name"  # Replace with your model name
URL = "your-api-url"       # Replace with your API endpoint
API_KEY = "your-api-key"   # Replace with your API key
----
To fill in these variables, use the information from your deployed model:

* Set `MODEL` to the name of your model (`base`).
* For the `URL`, check the `internal and external endpoints details` of your deployed model and use the **external endpoint**.
+
[.bordershadow]
image::04/04-01-inference-endpoints.png[link=self, window=blank, width=100%]
* Copy the model server `token` for the `API_KEY`.
+
[.bordershadow]
image::04/04-01-inference-token.png[link=self, window=blank, width=100%]

**Install the Required Dependency**: Go back to the created terminal and install the necessary package to interact with your model:
[source,bash]
----
pip install langchain_openai
----

=== Running the Script

Youâ€™re almost there! To run the script and measure its execution time, simply execute the following command in the terminal:
[source,bash]
----
time python request.py
----

Once you run the script, youâ€™ll see some exciting output, including:

* The script's output
* Real time (wall clock time)
* User CPU time
* System CPU time
+
[.bordershadow]
image::04/04-request-base.png[link=self, window=blank, width=100%]

This is your chance to see how well your model performs! ðŸš€


=== Remove model

When you're done testing, donâ€™t forget to clean up. Simply click on the **Delete** button in the **Models** tab to remove the model. 

IMPORTANT: ðŸš¨ Make sure to remove the model before proceeding to the next step to ensure you have enough GPUs available for your next tasks.
[.bordershadow]
image::04/04-01-model-delete.png[link=self, window=blank, width=100%]
[.bordershadow]
image::04/04-01-model-delete-base.png[link=self, window=blank, width=100%]
