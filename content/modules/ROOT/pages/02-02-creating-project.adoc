= Creating Your Project and Pipeline Server
include::_attributes.adoc[]

As a preliminary step, each of you is going to:

. ğŸš€ Create a Data Science Project
** This will help keep your things organized and ready for action!

. ğŸŒ Create a Data Connection
** We need that for the pipeline server to store its artifacts.

. ğŸ› ï¸ Deploy a Data Science Pipeline Server
** We will need one, and it's better to create it from the start.

. ğŸ’» Launch a Workbench
** We will use it to review content and notebooks and to run the lab exercises to optimize the model.

. ğŸ“¥ Clone the Git Repo into Your Workbench
** This contains all the code from the prototype, ready for you to explore!

The instructions below will guide you through these steps. Follow them carefully.

== ğŸŒŸ Create a Project

* First, in the OpenShift AI Dashboard application, navigate to the Data Science Projects menu on the left:
+
[.bordershadow]
image::02/02-02-ds-proj-nav.png[]

* Create a project with the **same name** as your user ID
** You have been assigned a unique user ID:  `{user}`
** You need to now create a project with the exact same name: `{user}`
+
IMPORTANT: ğŸš¨ Your assigned user is {user}. Don't mess that up or things will break later on!

* Leave the resource name unchanged.
* Optionally, enter your first and last name in the description of the project.
* It should look like this:
+
[.bordershadow]
image::02/02-02-create-project.png[]
+
IMPORTANT: ğŸš« It should **NOT** be `userX` like in the screenshot. (for you, `X` should be a number instead)

== ğŸŒˆ Create a Data Connection for the Pipeline Server

* We have deployed an instance of Minio in the cluster to act as a simple Object Storage for our purposes.
* You will need to **create a connection** that points to it.
+
[.bordershadow]
image::02/02-02-add-dc.png[]

* You need to select the connection type, in this case **S3 compatible object storage -v1**
+
[.bordershadow]
image::02/02-02-add-dc-type.png[]

* Here is the information you need to enter:
** Name:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
Minio - models
** Access Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{user}
** Secret Key:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{password}
** Endpoint:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
http://minio-service.wksp-{user}.svc.cluster.local:9000
** Region:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
none
** Bucket:
[.lines_space]
[.console-input]
[source, text]
[subs=attributes+]
{user}
+
IMPORTANT: ğŸš¨ Once again, the bucket you will use has to match with the user ID you were provided!

* The result should look similar to:
+
[.bordershadow]
image::02/data-connection.png[]


== ğŸ› ï¸ Create a Pipeline Server

It is highly recommended to create your pipeline server before creating a workbench. So let's do that now!

* In your Data Science Pipeline (project `{user}`), or in your Data Science Project, **Pipelines**, click on **Configure Pipeline Server**
+
[.bordershadow]
image::02/02-02-pipelineserver01.png[]

* Use the same information as in the Data Connection created earlier (**Minio - models**) and click the **Configure Pipeline Server** button:
+
[.bordershadow]
image::02/02-02-pipelineserver02.png[]

* When your pipeline server is ready, your screen will look like the following:
+
[.bordershadow]
image::02/02-02-pipelineserver03.png[]

At this point, your pipeline server is ready and deployed. ğŸ‰
