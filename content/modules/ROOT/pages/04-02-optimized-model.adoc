= Deploy and Test the Optimized Model

== Deploy the Optimized Model with VLLM

Now that we have the optimized models on the MinIO S3 bucket, let's deploy one of them.

To deploy a model you need to go to the created Data Science Project (`{user}`) and follow the nextd steps:

. **Navigate to Your Project**: Head over to your created Data Science Project and locate the **Models** section. 

. **Deploy Your Model**: Click on the **Deploy model** button to start the deployment process.
+
[.bordershadow]
image::04/04-01-deploy.png[link=self, window=blank, width=100%]
+
NOTE: Once single-model option has been selected for the Data Science project, there is no need to select that again, it gets annotated on the `namespace`.

. **Fill Out the Deployment Form**: Youâ€™ll need to provide some essential information. Hereâ€™s what to enter:
   * **Name**: `optimized`
   * **Serving runtime**: `vLLM ServingRuntime for KServe`
   * **Model server size**: `Small`
   * **Accelerator**: `NVIDIA GPU`
   * **Model route**: Select the option to make your model available through an external route.
   * **Token authentication**: Choose `Require token authentication` and leave the default **Service account name**.
   * **Existing connection**:
   ** **Connection**: `Minio - models`
   ** **Path**: `granite-int4` (you can choose also `granite-int8` or `granite-fp8`)

. **Deploy and Wait**: After filling out the form, click on **Deploy**. Now, wait while your model gets ready. This might take a moment! â˜•

== Test the Optimized Model

Now that the optimized model is deployed, it's time to put it to the test and compare it with the base mode. Get ready to send a request to your model and measure its response time.

=== Workbench Setup

We are going to reuse the workbench created in Section 4.1.

* Go back to the `terminal` workbench created in Section 4.1
* **Update Your Variables**: Open the `request.py` file and update the following variables to match your setup:
+
[source,python]
----
MODEL = "your-model-name"  // Replace with your model name
URL = "your-api-url"       // Replace with your API endpoint
API_KEY = "your-api-key"   // Replace with your API key
----
To fill in these variables, use the information from your deployed model:
** Set `MODEL` to the name of your model (`optimized`).
** For the `URL`, check the `internal and external endpoints details` of your deployed model and use the **external endpoint**.
+
[.bordershadow]
image::04/04-01-inference-endpoints.png[link=self, window=blank, width=100%]
** Copy the model server `token` for the `API_KEY`.
+
[.bordershadow]
image::04/04-01-inference-token.png[link=self, window=blank, width=100%]

* The required dependencies should already be installed from the previous Section, but if not install the next package in the previously created terminal:
+
[source,bash]
----
pip install langchain_openai
----

=== Running the Script

To run the script and measure its execution time, simply execute the following command in your terminal:
[source,bash]
----
time python request.py
----

Once you run the script, youâ€™ll see some exciting output, including:

* The script's output
* Real time (wall clock time)
* User CPU time
* System CPU time
+
[.bordershadow]
image::04/04-request-optimized.png[]

This is your chance to see how well your model performs in comparison with the base model! ðŸš€

=== Remove model

When you're done testing, donâ€™t forget to clean up. Simply click on the **Delete** button in the **Models** tab to remove the model. 

IMPORTANT: ðŸš¨ Make sure to remove the model before proceeding to the next step to ensure you have enough GPUs available for your next tasks.
[.bordershadow]
image::04/04-01-model-delete.png[link=self, window=blank, width=100%]
[.bordershadow]
image::04/04-02-model-delete-optimized.png[link=self, window=blank, width=100%]
