= Weights Only Quantization (INT4)

In this exercise, we will use the previously created workbench (Section 2) to investigate how LLMs weights can be quantized to **INT4** for memory savings and inference acceleration. This quantization method is particularly useful for:

- Reducing model size
- Maintaining low latency in workloads with low queries per second (QPS)

IMPORTANT: ðŸš¨ The time for the workshop is limited. Therefore, please do only one of the INT versions (either **INT4** or **INT8**) and then **FP8**, which is fast as it does not need calibration data. You may choose to skip this section or section 3.2.

== Quantization Process

The quantization process involves the following steps:

1. **Load the Model**: Load the pre-trained LLM model.
2. **Prepare Calibration Dataset**: Prepare a dataset for calibration.
3. **Quantize the Model**: Convert the model weights to **INT4** format.
   ** Using GPTQ
4. **Evaluate the Model**: Evaluate the quantized model's accuracy.

IMPORTANT: ðŸš¨ After quantizing the model, the GPU memory may not be freed. You need to **restart the kernel** before evaluating the model to ensure you have enough GPU RAM available.
[.bordershadow]
image::03/03-restart-kernel.png[title="Restart Kernel", link=self, window=blank, width=100%]


== Exercise: Quantize the Model with llm-compressor

Go to the workbench created in the previous section (Section 2). From the `showroom-summit2025-lb2959-neural-magic/lab-materials/03` folder, please open the notebook called `weight_only_quantization.ipynb` and follow the instructions.
[.bordershadow]
image::03/03-01-int4-notebook.png[title="Notebook", link=self, window=blank, width=100%]

To execute the cells you can select them and either click on the **play** icon or press **Shift + Enter**
[.bordershadow]
image::03/03-execute-cell.png[title="Execute", link=self, window=blank, width=100%]

When the cell is being executed, you can see **[*]**. And once the execution has completed, you will see a number instead of the *, e.g., **[1]**
[.bordershadow]
image::03/03-cell-status.png[title="Cell Status", link=self, window=blank, width=100%]

When done, you can close the notebook and head to the next page.

IMPORTANT: ðŸš¨ Once you complete all the quantization exercises and you no longer need the workbench, ensure you **stop it** so that the associated GPU gets freed and can be utilized to serve the model.
[.bordershadow]
image::03/03-workbench-done.png[title="Workbench Done", link=self, window=blank, width=100%]
[.bordershadow]
image::03/03-workbench-stop.png[title="Workbench Stop", link=self, window=blank, width=100%]